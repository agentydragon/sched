{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Explore Anki data",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agentydragon/sched/blob/master/Explore_Anki_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U67javkAzODA",
        "colab_type": "code",
        "outputId": "2b16200d-39e2-415c-ce45-e5f3e84bd491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datetime import timedelta\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6L9Fgt2zgto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conn = sqlite3.connect('/content/gdrive/My Drive/collection.anki2')\n",
        "data = []\n",
        "for row in conn.execute('SELECT id, cid, ease, time, type FROM revlog;'):\n",
        "  id, cid, ease, time, type = row\n",
        "  data.append((pd.Timestamp(id, unit='ms'), cid, ease, time, type))\n",
        "df = pd.DataFrame(data, columns = ['id', 'cid', 'ease', 'time', 'type'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMJ086KkCYGC",
        "colab_type": "text"
      },
      "source": [
        "Let's get the number of cards I have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raQj0FHXyI5I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ab887a8-301a-4070-86ee-b78a9d09ce42"
      },
      "source": [
        "print(len(pd.unique(df['cid'])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "34537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8jc2cu2CPWJ",
        "colab_type": "text"
      },
      "source": [
        "Forgetting curves\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ45dWes9B-7",
        "colab_type": "code",
        "outputId": "4aac438d-748b-4743-e04d-39cfe4fbd29c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "x = np.linspace(0.0, 10.0)\n",
        "plt.plot(x, np.exp(-x), x, np.exp(-0.5 * x), x, np.exp(-0.25 * x), x, np.exp(-0.05 * x))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f64d8701400>,\n",
              " <matplotlib.lines.Line2D at 0x7f64d8701550>,\n",
              " <matplotlib.lines.Line2D at 0x7f64d8701898>,\n",
              " <matplotlib.lines.Line2D at 0x7f64d8701be0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIp2W9gSUAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHNXUJxPDeoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_card_reviews(cid):\n",
        "  card_reviews = df[df.cid == cid]\n",
        "  card_reviews = card_reviews.loc[:, ['id', 'ease', 'time', 'type']]\n",
        "  card_reviews['correct'] = card_reviews.ease > 1\n",
        "  card_reviews = card_reviews.loc[:, ['id', 'correct']]\n",
        "  card_reviews.head()\n",
        "  card_reviews['diff'] = card_reviews['id'].diff()\n",
        "  return card_reviews[~card_reviews['diff'].isnull()]\n",
        "\n",
        "def get_sample_card_ids(n):\n",
        "  return pd.Series(pd.unique(df['cid'])).sort_values().sample(n=n, random_state=1).to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rN7g3WXf5gm",
        "colab_type": "text"
      },
      "source": [
        "Card difficulty - making lambda smaller/bigger on win/loss\n",
        "==="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZnbx0r99D_E",
        "colab_type": "text"
      },
      "source": [
        "Model: probability of getting a card right is $\\hat{p} = e^{-\\lambda \\cdot t}$.\n",
        "If you get a card right, $\\lambda$ gets multiplied by $w$, and if you get it wrong, it gets multiplied by $l$.\n",
        "The initial value of $\\lambda$ is $\\lambda_1$. $t$ is measured in floating-point days since last review. Parameters are $[\\lambda_1, w, l]$ and they are initialized with ones. To prevent taking logs of numbers very close to 0, we cap $\\hat{p}$ between 0.001 and 0.998. Here we try to fit one global model.\n",
        "\n",
        "We optimize $H(p, \\hat{p})$ by stochastic gradient descent. The model is trained on 1000 random cards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gubhQKRbfxuB",
        "colab_type": "code",
        "outputId": "0c881002-575e-41ae-fa32-740b9c37127b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1874
        }
      },
      "source": [
        "lambda1 = tf.Variable(1.0, name=\"lambda1\", trainable=True)\n",
        "win_p = tf.Variable(1.0)\n",
        "loss_p = tf.Variable(1.0)\n",
        "l = lambda1\n",
        "losses = []\n",
        "probabilities = []\n",
        "\n",
        "def build_losses_for_reviews(card_reviews):\n",
        "  l = lambda1\n",
        "  for index, row in card_reviews.iterrows():\n",
        "    #print(row)\n",
        "    diff_seconds = row['diff'].total_seconds()\n",
        "    diff_days = diff_seconds / 86400.0\n",
        "    current_probability = 0.998 * tf.exp(-l * diff_days) + 0.001\n",
        "    probabilities.append(current_probability)\n",
        "\n",
        "    if row['correct']:\n",
        "      # we predicted probability current_probability --> if it was 1, do nothing\n",
        "      loss_bit = -tf.log(current_probability)\n",
        "      losses.append(loss_bit)\n",
        "      l = l * win_p\n",
        "    else:\n",
        "      loss_bit = -tf.log(1.0 - current_probability)\n",
        "      losses.append(loss_bit)\n",
        "      l = l * loss_p\n",
        "      \n",
        "for card in get_sample_card_ids(1000):\n",
        "  build_losses_for_reviews(get_card_reviews(card))\n",
        "\n",
        "loss = tf.reduce_sum(losses) / len(losses)\n",
        "# Making training rate higher than 0.01 leads to shooting out to NaNs.\n",
        "opt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "variables = [lambda1, win_p, loss_p]\n",
        "train_op = opt.minimize(loss, var_list=variables)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for i in range(1000):\n",
        "    metadata = tf.RunMetadata()\n",
        "    sess.run(train_op)\n",
        "    if i % 10 == 0:\n",
        "      print(\"after round\", i, sess.run(variables), \"loss=\", sess.run(loss))\n",
        "  \n",
        "  print('Final:')\n",
        "  print(sess.run(variables))\n",
        "#  print('Probs: ', sess.run(probabilities))\n",
        "#  print('Loss: ', sess.run(loss))\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "after round 0 [0.9948127, 0.9265158, 0.9574966] loss= 2.7989676\n",
            "after round 10 [0.9554716, 0.68867004, 0.9956161] loss= 2.1950507\n",
            "after round 20 [0.9174773, 0.481618, 1.0700268] loss= 1.695402\n",
            "after round 30 [0.88792366, 0.32574418, 1.129935] loss= 1.406148\n",
            "after round 40 [0.8711519, 0.25430214, 1.1740621] loss= 1.3325257\n",
            "after round 50 [0.8594233, 0.21829039, 1.2124249] loss= 1.3035784\n",
            "after round 60 [0.8498727, 0.1991274, 1.2484754] loss= 1.285962\n",
            "after round 70 [0.84135073, 0.18910672, 1.2822889] loss= 1.2728657\n",
            "after round 80 [0.8332972, 0.18492456, 1.313216] loss= 1.2624648\n",
            "after round 90 [0.82545394, 0.18372652, 1.3425608] loss= 1.2532506\n",
            "after round 100 [0.8176308, 0.184057, 1.3704917] loss= 1.2448472\n",
            "after round 110 [0.8097611, 0.18538415, 1.3975385] loss= 1.2368796\n",
            "after round 120 [0.80181926, 0.18691508, 1.4235996] loss= 1.2294636\n",
            "after round 130 [0.79371446, 0.18824933, 1.4475842] loss= 1.2230388\n",
            "after round 140 [0.78545254, 0.19196247, 1.4714849] loss= 1.2165002\n",
            "after round 150 [0.776898, 0.19425538, 1.4935955] loss= 1.2108402\n",
            "after round 160 [0.76821333, 0.19777526, 1.5160556] loss= 1.2048961\n",
            "after round 170 [0.7593222, 0.20103462, 1.5387163] loss= 1.1988586\n",
            "after round 180 [0.75019246, 0.20364767, 1.5610934] loss= 1.1929554\n",
            "after round 190 [0.74085903, 0.20378019, 1.5812999] loss= 1.1880132\n",
            "after round 200 [0.73139876, 0.20320415, 1.5996745] loss= 1.1837499\n",
            "after round 210 [0.7218786, 0.20360534, 1.6171799] loss= 1.1797816\n",
            "after round 220 [0.7123076, 0.20506462, 1.634834] loss= 1.1757168\n",
            "after round 230 [0.7026262, 0.20578456, 1.652115] loss= 1.1717966\n",
            "after round 240 [0.6928912, 0.20386794, 1.6674023] loss= 1.1684824\n",
            "after round 250 [0.6832763, 0.2017405, 1.6813397] loss= 1.1655768\n",
            "after round 260 [0.6737732, 0.19982128, 1.69425] loss= 1.1629767\n",
            "after round 270 [0.664388, 0.19756012, 1.7059708] loss= 1.1606768\n",
            "after round 280 [0.65513986, 0.19567718, 1.7168326] loss= 1.1586096\n",
            "after round 290 [0.6459753, 0.19607657, 1.7277392] loss= 1.1565613\n",
            "after round 300 [0.63666934, 0.19805421, 1.7388307] loss= 1.1544269\n",
            "after round 310 [0.62715596, 0.19902977, 1.7491552] loss= 1.1524458\n",
            "after round 320 [0.6175738, 0.19842315, 1.7584177] loss= 1.1506683\n",
            "after round 330 [0.6080107, 0.19951843, 1.7678361] loss= 1.1488308\n",
            "after round 340 [0.59821206, 0.20396543, 1.7784035] loss= 1.1465538\n",
            "after round 350 [0.5880123, 0.20448917, 1.7876587] loss= 1.1446452\n",
            "after round 360 [0.57780087, 0.20322198, 1.795783] loss= 1.1429291\n",
            "after round 370 [0.5676877, 0.20197555, 1.803311] loss= 1.1413263\n",
            "after round 380 [0.55766827, 0.20104733, 1.8103667] loss= 1.1398176\n",
            "after round 390 [0.547712, 0.20079467, 1.8171072] loss= 1.1383715\n",
            "after round 400 [0.53774315, 0.20166796, 1.8237517] loss= 1.1369276\n",
            "after round 410 [0.5276889, 0.20274831, 1.830165] loss= 1.1354939\n",
            "after round 420 [0.5175623, 0.20430088, 1.8365498] loss= 1.1340352\n",
            "after round 430 [0.5073325, 0.20570639, 1.842727] loss= 1.1325878\n",
            "after round 440 [0.49706802, 0.20600146, 1.848348] loss= 1.1312184\n",
            "after round 450 [0.48686522, 0.20615691, 1.8536726] loss= 1.1298944\n",
            "after round 460 [0.4767335, 0.20663004, 1.8588828] loss= 1.1285946\n",
            "after round 470 [0.4666578, 0.2069601, 1.8638595] loss= 1.1273316\n",
            "after round 480 [0.45667508, 0.20693403, 1.868475] loss= 1.1261232\n",
            "after round 490 [0.44682425, 0.20689829, 1.8727915] loss= 1.1249676\n",
            "after round 500 [0.43711102, 0.20719427, 1.8769014] loss= 1.1238549\n",
            "after round 510 [0.42750153, 0.20861425, 1.8810337] loss= 1.1227366\n",
            "after round 520 [0.41787663, 0.21091068, 1.8851271] loss= 1.1215905\n",
            "after round 530 [0.4082349, 0.21250364, 1.8887758] loss= 1.1205032\n",
            "after round 540 [0.39865494, 0.21383905, 1.8921739] loss= 1.1194531\n",
            "after round 550 [0.3891794, 0.21461216, 1.8952122] loss= 1.1184579\n",
            "after round 560 [0.37988234, 0.21495502, 1.897886] loss= 1.1175221\n",
            "after round 570 [0.3708146, 0.2151793, 1.9002813] loss= 1.1166433\n",
            "after round 580 [0.36199838, 0.21540985, 1.902444] loss= 1.1158202\n",
            "after round 590 [0.35344434, 0.2156938, 1.9044002] loss= 1.1150506\n",
            "after round 600 [0.34515786, 0.21606976, 1.9061731] loss= 1.1143321\n",
            "after round 610 [0.33713877, 0.21660756, 1.907794] loss= 1.1136608\n",
            "after round 620 [0.32937774, 0.2174823, 1.9093223] loss= 1.1130282\n",
            "after round 630 [0.32184282, 0.21901163, 1.910853] loss= 1.1124133\n",
            "after round 640 [0.3144754, 0.22115079, 1.9123731] loss= 1.1118007\n",
            "after round 650 [0.30719778, 0.22510442, 1.9142525] loss= 1.1110716\n",
            "after round 660 [0.2997926, 0.22741033, 1.9155314] loss= 1.1104472\n",
            "after round 670 [0.29256842, 0.22806177, 1.9161619] loss= 1.1099181\n",
            "after round 680 [0.28568208, 0.22856674, 1.9166003] loss= 1.1094406\n",
            "after round 690 [0.27914876, 0.22906958, 1.9169018] loss= 1.1090114\n",
            "after round 700 [0.27296898, 0.2295785, 1.9170797] loss= 1.1086277\n",
            "after round 710 [0.2671404, 0.23009257, 1.9171445] loss= 1.108286\n",
            "after round 720 [0.26165864, 0.23061039, 1.9171052] loss= 1.1079837\n",
            "after round 730 [0.2565175, 0.23113035, 1.9169707] loss= 1.1077173\n",
            "after round 740 [0.25170898, 0.23165092, 1.916749] loss= 1.1074835\n",
            "after round 750 [0.24722353, 0.23217069, 1.9164478] loss= 1.1072793\n",
            "after round 760 [0.2430502, 0.23268841, 1.916074] loss= 1.1071016\n",
            "after round 770 [0.23917678, 0.233203, 1.9156344] loss= 1.1069475\n",
            "after round 780 [0.2355901, 0.23371336, 1.9151353] loss= 1.1068141\n",
            "after round 790 [0.23227614, 0.23421873, 1.9145825] loss= 1.1066991\n",
            "after round 800 [0.22922038, 0.23471843, 1.9139814] loss= 1.1066\n",
            "after round 810 [0.22640787, 0.23521179, 1.9133373] loss= 1.1065146\n",
            "after round 820 [0.22382356, 0.23569839, 1.9126549] loss= 1.106441\n",
            "after round 830 [0.22145239, 0.23617786, 1.9119384] loss= 1.1063774\n",
            "after round 840 [0.21927954, 0.23664999, 1.9111922] loss= 1.1063228\n",
            "after round 850 [0.21729048, 0.23711452, 1.9104198] loss= 1.106275\n",
            "after round 860 [0.21547109, 0.23757142, 1.9096249] loss= 1.1062336\n",
            "after round 870 [0.21380788, 0.23802054, 1.9088107] loss= 1.1061975\n",
            "after round 880 [0.21228804, 0.23846203, 1.90798] loss= 1.1061656\n",
            "after round 890 [0.2108993, 0.23889595, 1.9071354] loss= 1.1061375\n",
            "after round 900 [0.20963018, 0.23932241, 1.906279] loss= 1.1061122\n",
            "after round 910 [0.20846999, 0.23974165, 1.9054129] loss= 1.1060895\n",
            "after round 920 [0.20740868, 0.24015383, 1.9045393] loss= 1.1060691\n",
            "after round 930 [0.20643702, 0.2405591, 1.90366] loss= 1.1060501\n",
            "after round 940 [0.2055465, 0.24095768, 1.9027765] loss= 1.1060328\n",
            "after round 950 [0.20472924, 0.24134983, 1.90189] loss= 1.1060169\n",
            "after round 960 [0.20397808, 0.2417358, 1.9010019] loss= 1.1060017\n",
            "after round 970 [0.20328644, 0.24211563, 1.9001138] loss= 1.1059877\n",
            "after round 980 [0.2026484, 0.24248976, 1.8992257] loss= 1.1059744\n",
            "after round 990 [0.20205858, 0.2428584, 1.8983392] loss= 1.1059618\n",
            "Final:\n",
            "[0.20156488, 0.2431856, 1.8975431]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCek4Tnh-HCM",
        "colab_type": "text"
      },
      "source": [
        "One training gave the result: $\\lambda_1\\approx 0.2016$, $w\\approx 0.2432$, $l\\approx 1.8975$.\n",
        "\n",
        "Looks like we are bottoming out at a cross-entropy of about 1.1 on each review. From $-\\log(x)=1.1$, we have $x=e^{-1.1}\\approx 0.33$. So that's about as much entropy as if we predicted the probability of a true event at $1/3$.\n",
        "\n",
        "Questions:\n",
        "\n",
        "* What if I let this run longer?\n",
        "* Can I do constrained optimization in Tensorflow, so that $\\lambda$ in model below is fixed to positive?\n",
        "* Would other optimizers be faster?\n",
        "\n",
        "Extension ideas:\n",
        "\n",
        "* Would it be useful to model per-card state?\n",
        "* Compute test-set loss on a withheld set of cards\n",
        "* Operate on batches; construct one computation graph for one card, do not throw them all into the graph.\n",
        "* Anki's SR algorithm uses the *interval*, the *card difficulty* and the *number of last successful reviews*. Would integrating the number of last successful reviews into the model help?\n",
        "* It would be nice to get a visualization of how the probabilities look for a given card, with review points.\n",
        "* It might be helpful to distinguish between \"learn\" reviews, \"relearn\" reviews and others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuC5QxS-fysR",
        "colab_type": "text"
      },
      "source": [
        "Basic model - the same forgetting curve starting after every revision\n",
        "==="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPokEB70-puc",
        "colab_type": "text"
      },
      "source": [
        "This one currently does not work, and tends to somehow run into NaN's. I should fix that at some point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIk8oIoiCFf_",
        "colab_type": "code",
        "outputId": "9d95e172-5b29-4397-8c8d-0e430ab845cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "lambda1 = tf.Variable(1.0, name=\"lambda1\", trainable=True)\n",
        "losses = []\n",
        "probabilities = []\n",
        "\n",
        "def build_losses_for_reviews(card_reviews):\n",
        "  l = lambda1\n",
        "  for index, row in card_reviews.iterrows():\n",
        "    #print(row)\n",
        "    diff_seconds = row['diff'].total_seconds()\n",
        "    diff_days = diff_seconds / 86400.0\n",
        "    current_probability = 0.9998 * tf.exp(-l * diff_days) + 0.0001\n",
        "    probabilities.append(current_probability)\n",
        "\n",
        "    if row['correct']:\n",
        "      # we predicted probability current_probability --> if it was 1, do nothing\n",
        "      loss_bit = -tf.log(current_probability)\n",
        "      losses.append(loss_bit)\n",
        "    else:\n",
        "      loss_bit = -tf.log(1.0 - current_probability)\n",
        "      losses.append(loss_bit)\n",
        "\n",
        "for card in get_sample_card_ids(10):\n",
        "  build_losses_for_reviews(get_card_reviews(card))\n",
        "\n",
        "loss = tf.reduce_sum(losses) / len(losses)\n",
        "opt = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "variables = [lambda1, lambda2]\n",
        "train_op = opt.minimize(loss, var_list=variables)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for i in range(100):\n",
        "    loss_value = sess.run(loss)\n",
        "    if np.isnan(loss_value):\n",
        "      print('Reached NaN!')\n",
        "    sess.run(train_op)\n",
        "    if i % 10 == 0:\n",
        "      print('Probs: ', sess.run(probabilities))\n",
        "      print('Loss: ', loss_value)\n",
        "      print(\"after round\", i, sess.run(variables))\n",
        "  \n",
        "  print('Final:')\n",
        "  print(sess.run(variables))\n",
        "  print('Probs: ', sess.run(probabilities))\n",
        "  print('Loss: ', loss_value)\n",
        "    \n",
        "    \n",
        "# TODO: why does the lambda get optimized down into negative numbers? >:("
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probs:  [0.983854, 0.9731835, 0.6304561, 0.6378329, 0.00010707023, 0.00010000005, 1e-04, 0.9870601, 0.98080695, 0.00013492166, 0.8915018, 0.80357873, 0.11786603, 0.018518459, 0.0012241048, 0.9928614, 0.9825855, 0.33358455, 0.9917703, 0.9197108, 0.83498573, 0.08052481, 0.12274151, 0.062798, 0.0043989536, 0.00038960838, 0.000100002035, 1e-04, 1e-04, 0.9921068, 0.91639555, 1e-04, 1e-04, 1e-04, 0.94125444, 0.52467954, 0.00011422679, 1e-04, 1e-04, 0.99875134, 0.9923497, 0.98533535, 0.018801449, 0.0022194902, 0.000100337245, 1e-04, 1e-04, 0.9986431, 0.69440746, 0.6172623, 0.00010000204, 1e-04, 1e-04, 0.9929457, 0.6246892, 0.00010000494, 1e-04, 1e-04, 0.9922447, 0.95966107, 0.20310037, 0.05310323, 0.000100000056, 1e-04, 1e-04, 0.9995254, 0.9995709, 0.62954056, 0.014662209, 0.00034591663, 0.00010002474, 1e-04, 1e-04, 0.85582453, 0.89602727, 0.8652565, 0.00012321361, 0.992933, 0.99084216, 0.9657137, 0.00010600637, 0.98374814, 0.98277557, 0.15442748, 0.0008701521, 0.00022042947, 0.00010003694, 1e-04, 1e-04, 1e-04, 0.99916565, 0.99204767, 0.98255646, 0.000100376885, 0.00010000003, 1e-04, 1e-04, 0.98831, 0.8771266, 0.024217416, 1e-04, 1e-04]\n",
            "Loss:  4.0319557\n",
            "after round 0 [0.9993052, 1.0]\n",
            "Probs:  [0.9839651, 0.9733676, 0.6324894, 0.639838, 0.00010768054, 0.000100000056, 1e-04, 0.98714924, 0.9809391, 0.00013751545, 0.89221627, 0.8048059, 0.11963778, 0.019039303, 0.0012786803, 0.9929103, 0.98270535, 0.33615074, 0.9918269, 0.9202476, 0.83603716, 0.0819524, 0.12455136, 0.06402197, 0.004565652, 0.0004065579, 0.00010000234, 1e-04, 1e-04, 0.992161, 0.91695374, 1e-04, 1e-04, 1e-04, 0.94165164, 0.5270469, 0.0001153796, 1e-04, 1e-04, 0.9987594, 0.99240226, 0.9854363, 0.019328244, 0.0023125743, 0.00010037422, 1e-04, 1e-04, 0.9986519, 0.69617724, 0.6193445, 0.000100002355, 1e-04, 1e-04, 0.99299407, 0.6267441, 0.00010000564, 1e-04, 1e-04, 0.9922979, 0.9599363, 0.20537262, 0.054201342, 0.00010000007, 1e-04, 1e-04, 0.99952805, 0.99957323, 0.63157725, 0.01509858, 0.0003606064, 0.00010002796, 1e-04, 1e-04, 0.8567547, 0.89671355, 0.86613065, 0.00012500901, 0.99298143, 0.9909051, 0.9659482, 0.00010653229, 0.98386, 0.9828941, 0.15645386, 0.0009096781, 0.00022826108, 0.00010004163, 1e-04, 1e-04, 1e-04, 0.9991708, 0.99210227, 0.98267657, 0.000100417885, 0.00010000003, 1e-04, 1e-04, 0.98839045, 0.87792915, 0.024852784, 1e-04, 1e-04]\n",
            "Loss:  4.027094\n",
            "after round 10 [0.99232846, 1.0]\n",
            "Probs:  [0.9840771, 0.97355294, 0.63454455, 0.64186454, 0.000108348744, 0.00010000007, 1e-04, 0.9872389, 0.98107207, 0.00014032371, 0.8929367, 0.8060443, 0.12144981, 0.019578962, 0.0013363472, 0.9929597, 0.98282605, 0.33875608, 0.9918839, 0.92078894, 0.83709776, 0.08341639, 0.12640184, 0.06527931, 0.004740146, 0.00042463842, 0.0001000027, 1e-04, 1e-04, 0.9922156, 0.91751635, 1e-04, 1e-04, 1e-04, 0.9420521, 0.52944297, 0.00011663557, 1e-04, 1e-04, 0.99876744, 0.9924551, 0.985538, 0.019874016, 0.0024104947, 0.00010041558, 1e-04, 1e-04, 0.99866074, 0.69796497, 0.62144935, 0.00010000271, 1e-04, 1e-04, 0.9930428, 0.6288213, 0.00010000646, 1e-04, 1e-04, 0.9923516, 0.9602137, 0.20768766, 0.055330735, 0.00010000008, 1e-04, 1e-04, 0.9995307, 0.99957556, 0.633636, 0.015551467, 0.00037629457, 0.000100031626, 1e-04, 1e-04, 0.85769284, 0.8974057, 0.8670122, 0.00012695839, 0.99303025, 0.9909686, 0.9661846, 0.00010710874, 0.9839727, 0.98301363, 0.15852241, 0.0009515539, 0.00023666673, 0.00010004696, 1e-04, 1e-04, 1e-04, 0.99917597, 0.9921573, 0.9827975, 0.00010046371, 0.000100000034, 1e-04, 1e-04, 0.9884715, 0.8787385, 0.025509862, 1e-04, 1e-04]\n",
            "Loss:  4.022158\n",
            "after round 20 [0.9852992, 1.0]\n",
            "Probs:  [0.9841899, 0.9737399, 0.6366221, 0.64391303, 0.00010908081, 0.000100000085, 1e-04, 0.98732936, 0.9812062, 0.00014336582, 0.89366317, 0.80729383, 0.12330339, 0.020138264, 0.0013973047, 0.99300945, 0.9829478, 0.34140173, 0.9919413, 0.92133474, 0.83816785, 0.08491796, 0.12829417, 0.0665712, 0.0049228556, 0.00044393467, 0.000100003104, 1e-04, 1e-04, 0.9922706, 0.91808367, 1e-04, 1e-04, 1e-04, 0.9424558, 0.5318682, 0.00011800485, 1e-04, 1e-04, 0.99877554, 0.9925085, 0.9856405, 0.020439593, 0.0025135402, 0.000100461875, 1e-04, 1e-04, 0.9986696, 0.6997708, 0.62357754, 0.00010000312, 1e-04, 1e-04, 0.993092, 0.6309212, 0.0001000074, 1e-04, 1e-04, 0.9924056, 0.9604933, 0.21004666, 0.05649254, 0.00010000009, 1e-04, 1e-04, 0.99953336, 0.9995779, 0.6357171, 0.016021611, 0.00039305686, 0.00010003581, 1e-04, 1e-04, 0.8586392, 0.8981036, 0.8679014, 0.00012907624, 0.99307954, 0.9910325, 0.9664228, 0.00010774103, 0.98408633, 0.983134, 0.1606343, 0.0009959375, 0.0002456934, 0.00010005301, 1e-04, 1e-04, 1e-04, 0.99918115, 0.9922128, 0.9829194, 0.00010051496, 0.00010000004, 1e-04, 1e-04, 0.98855317, 0.8795548, 0.02618956, 1e-04, 1e-04]\n",
            "Loss:  4.017148\n",
            "after round 30 [0.9782167, 1.0]\n",
            "Probs:  [0.9843036, 0.9739283, 0.6387224, 0.64598376, 0.00010988343, 0.0001000001, 1e-04, 0.9874205, 0.9813412, 0.00014666346, 0.8943958, 0.80855507, 0.12519976, 0.020718098, 0.0014617703, 0.9930595, 0.98307043, 0.3440885, 0.99199915, 0.92188495, 0.8392474, 0.08645842, 0.13022967, 0.067898884, 0.005114245, 0.0004645392, 0.000100003585, 1e-04, 1e-04, 0.9923261, 0.9186556, 1e-04, 1e-04, 1e-04, 0.94286263, 0.53432316, 0.00011949864, 1e-04, 1e-04, 0.99878377, 0.9925622, 0.9857438, 0.021025864, 0.0026220244, 0.00010051374, 1e-04, 1e-04, 0.99867857, 0.70159525, 0.6257293, 0.0001000036, 1e-04, 1e-04, 0.9931415, 0.63304424, 0.00010000848, 1e-04, 1e-04, 0.9924601, 0.9607751, 0.21245088, 0.05768797, 0.000100000114, 1e-04, 1e-04, 0.99953604, 0.99958014, 0.637821, 0.016509835, 0.0004109763, 0.00010004058, 1e-04, 1e-04, 0.85959387, 0.89880735, 0.86879826, 0.00013137865, 0.9931292, 0.9910969, 0.966663, 0.00010843505, 0.9842008, 0.98325527, 0.16279085, 0.0010430014, 0.00025539228, 0.0001000599, 1e-04, 1e-04, 1e-04, 0.99918634, 0.9922686, 0.98304224, 0.00010057233, 0.00010000005, 1e-04, 1e-04, 0.98863554, 0.88037807, 0.026892867, 1e-04, 1e-04]\n",
            "Loss:  4.01206\n",
            "after round 40 [0.97107995, 1.0]\n",
            "Probs:  [0.9844183, 0.9741182, 0.64084595, 0.64807713, 0.00011076402, 0.000100000114, 1e-04, 0.9875124, 0.98147744, 0.00015024022, 0.89513475, 0.809828, 0.12714028, 0.021319363, 0.0015299724, 0.99311006, 0.983194, 0.34681734, 0.9920573, 0.9224397, 0.8403367, 0.08803901, 0.13220967, 0.06926363, 0.005314793, 0.0004865511, 0.00010000414, 1e-04, 1e-04, 0.99238193, 0.9192323, 1e-04, 1e-04, 1e-04, 0.9432729, 0.5368086, 0.000121129306, 1e-04, 1e-04, 0.99879205, 0.99261636, 0.98584795, 0.021633746, 0.0027362758, 0.0001005719, 1e-04, 1e-04, 0.9986876, 0.7034386, 0.6279052, 0.00010000416, 1e-04, 1e-04, 0.99319136, 0.63519084, 0.00010000973, 1e-04, 1e-04, 0.992515, 0.9610591, 0.21490142, 0.05891823, 0.000100000136, 1e-04, 1e-04, 0.9995387, 0.9995826, 0.6399483, 0.017016955, 0.0004301422, 0.00010004603, 1e-04, 1e-04, 0.8605569, 0.8995172, 0.869703, 0.00013388324, 0.99317914, 0.9911618, 0.9669051, 0.00010919737, 0.9843161, 0.9833775, 0.16499332, 0.0010929285, 0.0002658192, 0.00010006775, 1e-04, 1e-04, 1e-04, 0.99919164, 0.9923249, 0.983166, 0.00010063662, 0.00010000006, 1e-04, 1e-04, 0.98871845, 0.8812084, 0.027620759, 1e-04, 1e-04]\n",
            "Loss:  4.006894\n",
            "after round 50 [0.9638882, 1.0]\n",
            "Probs:  [0.98453385, 0.97430944, 0.64299315, 0.65019363, 0.00011173085, 0.000100000136, 1e-04, 0.9876049, 0.98161465, 0.00015412219, 0.89588, 0.8111129, 0.12912637, 0.021943051, 0.0016021611, 0.9931609, 0.98331857, 0.34958932, 0.99211615, 0.92299914, 0.84143597, 0.0896612, 0.13423552, 0.07066678, 0.005525015, 0.00051007903, 0.000100004785, 1e-04, 1e-04, 0.99243826, 0.91981393, 1e-04, 1e-04, 1e-04, 0.9436865, 0.53932506, 0.00012291066, 1e-04, 1e-04, 0.9988004, 0.9926709, 0.9859529, 0.022264224, 0.0028566543, 0.000100637175, 1e-04, 1e-04, 0.99869674, 0.70530117, 0.6301056, 0.00010000481, 1e-04, 1e-04, 0.99324155, 0.6373616, 0.00010001118, 1e-04, 1e-04, 0.9925703, 0.9613455, 0.21739964, 0.060184646, 0.00010000016, 1e-04, 1e-04, 0.9995414, 0.999585, 0.6420992, 0.01754388, 0.00045065233, 0.00010005227, 1e-04, 1e-04, 0.86152846, 0.900233, 0.8706158, 0.00013660963, 0.9932294, 0.99122715, 0.967149, 0.00011003534, 0.98443234, 0.9835007, 0.16724306, 0.0011459189, 0.00027703505, 0.00010007671, 1e-04, 1e-04, 1e-04, 0.999197, 0.9923817, 0.9832908, 0.00010070871, 0.00010000007, 1e-04, 1e-04, 0.9888021, 0.8820461, 0.028374324, 1e-04, 1e-04]\n",
            "Loss:  4.001647\n",
            "after round 60 [0.9566406, 1.0]\n",
            "Probs:  [0.9846502, 0.9745023, 0.64516443, 0.65233374, 0.00011279314, 0.000100000165, 1e-04, 0.98769826, 0.98175305, 0.00015833817, 0.8966318, 0.81240976, 0.13115942, 0.02259018, 0.0016786045, 0.99321216, 0.98344415, 0.3524055, 0.99217534, 0.92356324, 0.8425452, 0.09132635, 0.13630868, 0.07210974, 0.0057454607, 0.0005352407, 0.00010000554, 1e-04, 1e-04, 0.992495, 0.92040044, 1e-04, 1e-04, 1e-04, 0.9441035, 0.54187316, 0.00012485795, 1e-04, 1e-04, 0.9988088, 0.9927259, 0.9860586, 0.022918325, 0.0029835375, 0.00010071051, 1e-04, 1e-04, 0.998706, 0.70718324, 0.63233113, 0.00010000556, 1e-04, 1e-04, 0.9932923, 0.63955677, 0.00010001286, 1e-04, 1e-04, 0.992626, 0.96163416, 0.21994688, 0.0614886, 0.00010000019, 1e-04, 1e-04, 0.99954414, 0.99958736, 0.6442743, 0.018091543, 0.00047261282, 0.000100059406, 1e-04, 1e-04, 0.8625089, 0.9009551, 0.8715367, 0.00013957938, 0.99328023, 0.99129313, 0.96739495, 0.00011095715, 0.9845496, 0.9836249, 0.16954154, 0.0012021863, 0.00028910674, 0.00010008693, 1e-04, 1e-04, 1e-04, 0.9992024, 0.99243885, 0.9834165, 0.00010078963, 0.00010000009, 1e-04, 1e-04, 0.9888863, 0.8828912, 0.029154675, 1e-04, 1e-04]\n",
            "Loss:  3.996318\n",
            "after round 70 [0.9493361, 1.0]\n",
            "Probs:  [0.9847676, 0.9746968, 0.6473603, 0.654498, 0.000113961236, 0.000100000194, 1e-04, 0.9877923, 0.9818924, 0.00016292004, 0.8973901, 0.81371915, 0.13324106, 0.02326185, 0.0017595915, 0.9932639, 0.98357075, 0.35526696, 0.992235, 0.9241323, 0.8436648, 0.09303606, 0.13843074, 0.073594, 0.005976722, 0.0005621648, 0.00010000642, 1e-04, 1e-04, 0.9925523, 0.92099196, 1e-04, 1e-04, 1e-04, 0.944524, 0.5444536, 0.00012698825, 1e-04, 1e-04, 0.9988172, 0.9927814, 0.98616517, 0.023597151, 0.003117341, 0.00010079296, 1e-04, 1e-04, 0.9987152, 0.7090854, 0.6345822, 0.00010000645, 1e-04, 1e-04, 0.99334335, 0.6417771, 0.000100014804, 1e-04, 1e-04, 0.9926823, 0.96192527, 0.22254454, 0.062831536, 0.00010000022, 1e-04, 1e-04, 0.99954695, 0.99958974, 0.6464741, 0.018660963, 0.00049613987, 0.00010006759, 1e-04, 1e-04, 0.86349815, 0.90168345, 0.87246585, 0.00014281654, 0.9933314, 0.99135965, 0.9676429, 0.00011197199, 0.9846677, 0.9837501, 0.17189023, 0.0012619648, 0.00030210734, 0.000100098616, 1e-04, 1e-04, 1e-04, 0.99920774, 0.9924965, 0.9835433, 0.00010088054, 0.00010000011, 1e-04, 1e-04, 0.9889713, 0.8837437, 0.02996302, 1e-04, 1e-04]\n",
            "Loss:  3.9909039\n",
            "after round 80 [0.9419736, 1.0]\n",
            "Probs:  [0.9848859, 0.9748929, 0.64958143, 0.6566869, 0.00011524663, 0.00010000024, 1e-04, 0.9878871, 0.9820331, 0.00016790276, 0.89815515, 0.815041, 0.1353728, 0.023959193, 0.0018454314, 0.993316, 0.9836983, 0.35817486, 0.9922952, 0.92470616, 0.8447948, 0.09479189, 0.14060324, 0.07512112, 0.006219423, 0.0005909902, 0.00010000745, 1e-04, 1e-04, 0.99260986, 0.9215886, 1e-04, 1e-04, 1e-04, 0.944948, 0.5470673, 0.0001293204, 1e-04, 1e-04, 0.9988257, 0.99283725, 0.9862727, 0.024301847, 0.0032584965, 0.00010088576, 1e-04, 1e-04, 0.9987245, 0.71100795, 0.63685954, 0.000100007484, 1e-04, 1e-04, 0.9933949, 0.64402294, 0.00010001707, 1e-04, 1e-04, 0.9927389, 0.9622187, 0.22519405, 0.06421494, 0.00010000027, 1e-04, 1e-04, 0.9995497, 0.99959224, 0.6486992, 0.019253172, 0.0005213584, 0.00010007698, 1e-04, 1e-04, 0.8644965, 0.9024182, 0.8734034, 0.00014634764, 0.993383, 0.9914266, 0.96789294, 0.00011309008, 0.9847868, 0.9838763, 0.17429066, 0.0013255026, 0.00031611643, 0.000100111974, 1e-04, 1e-04, 1e-04, 0.9992132, 0.9925546, 0.98367107, 0.00010098278, 0.00010000013, 1e-04, 1e-04, 0.9890569, 0.8846039, 0.030800594, 1e-04, 1e-04]\n",
            "Loss:  3.9854028\n",
            "after round 90 [0.93455225, 1.0]\n",
            "Final:\n",
            "[0.9278214, 1.0]\n",
            "Probs:  [0.9849933, 0.97507066, 0.65160245, 0.6586784, 0.00011651449, 0.000100000274, 1e-04, 0.98797315, 0.9821606, 0.00017276232, 0.8988496, 0.81624186, 0.13733567, 0.024609787, 0.0019271182, 0.99336326, 0.98381406, 0.36083275, 0.9923497, 0.92522705, 0.8458211, 0.09641302, 0.14260307, 0.07653356, 0.006448197, 0.0006186852, 0.000100008525, 1e-04, 1e-04, 0.9926622, 0.92213017, 1e-04, 1e-04, 1e-04, 0.9453328, 0.54944855, 0.00013160935, 1e-04, 1e-04, 0.9988335, 0.992888, 0.9863701, 0.024959235, 0.0033922226, 0.00010097928, 1e-04, 1e-04, 0.9987329, 0.71275616, 0.63893193, 0.00010000857, 1e-04, 1e-04, 0.9934416, 0.64606667, 0.00010001941, 1e-04, 1e-04, 0.99279034, 0.96248496, 0.22762431, 0.065496, 0.00010000032, 1e-04, 1e-04, 0.99955225, 0.99959445, 0.6507239, 0.019806601, 0.0005456163, 0.00010008662, 1e-04, 1e-04, 0.86540294, 0.9030852, 0.8742547, 0.00014980134, 0.9934298, 0.9914874, 0.9681198, 0.00011419419, 0.9848949, 0.9839908, 0.17649673, 0.0013861279, 0.00032966016, 0.00010012566, 1e-04, 1e-04, 1e-04, 0.99921817, 0.9926073, 0.9837871, 0.00010108574, 0.00010000015, 1e-04, 1e-04, 0.98913455, 0.88538486, 0.031580538, 1e-04, 1e-04]\n",
            "Loss:  3.980375\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}